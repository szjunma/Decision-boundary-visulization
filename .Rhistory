# names(train)
# cat("test data column names after slight feature engineering\n")
# names(test)
set.seed(9)
gc()
tra<-train[,feature.names]
#tra<-tra[,c(1:50,65:105,110:165,180:230,245:290)]
dim(tra)
dim(test)
nrow(train)
h<-sample(nrow(train),100000)
dval<-xgb.DMatrix(data=data.matrix(tra[h,]),label=train$QuoteConversion_Flag[h])
#dtrain<-xgb.DMatrix(data=data.matrix(tra[-h,]),label=train$QuoteConversion_Flag[-h])
dtrain<-xgb.DMatrix(data=data.matrix(tra[,]),label=train$QuoteConversion_Flag)
watchlist<-list(val=dval,train=dtrain)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
s <- Sys.time()
s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 30,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
setwd("~/")
install.packages('xgboost/R-package/', repos=NULL, type='source')
cl <- makeCluster(detectCores())
registerDoParallel(cl)
s <- Sys.time()
s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
install.packages('xgboost/R-package/', repos=NULL, type='source')
cl <- makeCluster(detectCores())
registerDoParallel(cl)
s <- Sys.time()
s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
library(xgboost)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", #nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
setwd("~/Dropbox/Data project/Kaggle Homesite")
library(readr)
library(xgboost)
library(doParallel)
library(caret)
set.seed(308)
cat("reading the train and test data\n")
train <- read_csv("input/train.csv")
test  <- read_csv("input/test.csv")
# There are some NAs in the integer columns so conversion to zero
train[is.na(train)]   <- -1
test[is.na(test)]   <- -1
# cat("train data column names and details\n")
# names(train)
# str(train)
# summary(train)
# cat("test data column names and details\n")
# names(test)
# str(test)
# summary(test)
# seperating out the elements of the date column for the train set
train$month <- as.integer(format(train$Original_Quote_Date, "%m"))
train$year <- as.integer(format(train$Original_Quote_Date, "%y"))
train$day <- weekdays(as.Date(train$Original_Quote_Date))
# removing the date column
train <- train[,-c(2)]
# seperating out the elements of the date column for the train set
test$month <- as.integer(format(test$Original_Quote_Date, "%m"))
test$year <- as.integer(format(test$Original_Quote_Date, "%y"))
test$day <- weekdays(as.Date(test$Original_Quote_Date))
# removing the date column
test <- test[,-c(2)]
feature.names <- names(train)[c(3:301)]
# cat("Feature Names\n")
# feature.names
cat("assuming text variables are categorical & replacing them with numeric ids\n")
for (f in feature.names) {
if (class(train[[f]])=="character") {
levels <- unique(c(train[[f]], test[[f]]))
train[[f]] <- as.integer(factor(train[[f]], levels=levels))
test[[f]]  <- as.integer(factor(test[[f]],  levels=levels))
}
}
# cat("train data column names after slight feature engineering\n")
# names(train)
# cat("test data column names after slight feature engineering\n")
# names(test)
set.seed(9)
gc()
tra<-train[,feature.names]
#tra<-tra[,c(1:50,65:105,110:165,180:230,245:290)]
dim(tra)
dim(test)
nrow(train)
h<-sample(nrow(train),100000)
dval<-xgb.DMatrix(data=data.matrix(tra[h,]),label=train$QuoteConversion_Flag[h])
#dtrain<-xgb.DMatrix(data=data.matrix(tra[-h,]),label=train$QuoteConversion_Flag[-h])
dtrain<-xgb.DMatrix(data=data.matrix(tra[,]),label=train$QuoteConversion_Flag)
watchlist<-list(val=dval,train=dtrain)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
s <- Sys.time()
s
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", #nthread = 8,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
# xgb_grid_1 = expand.grid( nrounds = 100, eta = c(0.01), max_depth = c(10),
#                           gamma = 1, colsample_bytree = 0.66, min_child_weight =1)
#
# # pack the training control parameters
# xgb_trcontrol_1 = trainControl(method = "cv", number = 5, verboseIter = TRUE, returnData = FALSE,
#                                returnResamp = "all", # save losses across all models
#                                classProbs = TRUE,  # set to TRUE for AUC to be computed
#                                summaryFunction = twoClassSummary,
#                                allowParallel = TRUE
# )
#
# xgb_train_1 = train(QuoteConversion_Flag~., data = train[1:1000, -1],
#   trControl = xgb_trcontrol_1,
#   tuneGrid = xgb_grid_1,
#   method = "xgbTree"
# )
stopCluster(cl)
Sys.time() - s
detach("package:doParallel", unload=TRUE)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 4,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 10,
verbose = T, maximize = FALSE)
param <- list(objective = "binary:logistic", booster = "gbtree", eval_metric = "auc", nthread = 4,
eta = 0.01, # 0.06, #0.01,
max_depth = 10, #changed from default of 8
subsample = 0.85, # 0.7
colsample_bytree = 0.66 # 0.7
#num_parallel_tree   = 2
# alpha = 0.0001,
# lambda = 1
)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
install.packages(“Rcpp”)
install.packages('Rcpp')
install.packages('xgboost/R-package/', type='source')
setwd("~/")
install.packages('xgboost/R-package/', type='source')
install.packages('xgboost/R-package/', repos=NULL, type='source')
devtools::install_github("hadley/devtools")
install_github("hadley/devtools")
install.packages("devtools")
devtools::install_local('xgboost/', subdir = 'R-package')
is_using_multithread()
install.packages('mxnet/R-package/', repos=NULL, type='source')
devtools::install_local('mxnet/', subdir = 'R-package')
library(mxnet)
devtools::install_local('mxnet/', subdir = 'R-package')
install.packages('mxnet/R-package/', repos=NULL, type='source')
install.packages('mxnet/R-package/', repos=NULL, type='source')
library(mxnet)
demo(topic = "basic_bench", package = "mxnet")
print(paste0(“Finish prediction… accuracy=”, accuracy(label, pred)))
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
install.packages("forecast")
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
library(forecast)
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
demo(topic = "basic_bench", package = "mxnet")
print(paste0('Finish prediction… accuracy=', accuracy(label, pred)))
demo(topic = "basic_model", package = "mxnet")
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
library(xgboost)
devtools::install_local('xgboost/', subdir = 'R-package')
setwd("~/")
devtools::install_local('xgboost/', subdir = 'R-package')
library(xgboost)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
dtrain<-xgb.DMatrix(data=data.matrix(tra[,]),label=train$QuoteConversion_Flag)
clf <- xgboost(params = param, data = dtrain, nrounds = 20,
verbose = T, maximize = FALSE)
?n
library(ggmap)
library(ggplot2)
library(dplyr)
library(readr)
?n
install.packages('devtools', repo=\ 'https://cran.rstudio.com')
install.packages('devtools', repo=\'https://cran.rstudio.com')
install.packages('devtools', repo='https://cran.rstudio.com')
library(mxnet)
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv()
dyn.load('/usr/local/cuda/lib/libcudart.7.5.dylib')
dyn.load('/usr/local/cuda/lib/libcublas.dylib')
dyn.load('/usr/local/cuda/lib/libcurand.dylib')
Sys.getenv("LD_LIBRARY_PATH")
Sys.getenv("DYLD_LIBRARY_PATH")
dyn.load('/usr/local/cuda/lib/libcudart.7.5.dylib')
dyn.load('/usr/local/cuda/lib/libcublas.dylib')
dyn.load('/usr/local/cuda/lib/libcurand.dylib')
?dyn.load
?n.load
library(mxnet)
require(mxnet)
context("ndarray")
test_that("element-wise calculation for vector", {
x = 1:10
mat = mx.nd.array(as.array(x), mx.cpu(0))
expect_equal(x, as.array(mat))
expect_equal(x+1, as.array(mat+1))
expect_equal(x-10, as.array(mat-10))
expect_equal(x*20, as.array(mat*20))
expect_equal(x/3, as.array(mat/3), tolerance = 1e-5)
expect_equal(-1-x, as.array(-1-mat))
expect_equal(-5/x, as.array(-5/mat), tolerance = 1e-5)
expect_equal(x+x, as.array(mat+mat))
expect_equal(x/x, as.array(mat/mat))
expect_equal(x*x, as.array(mat*mat))
expect_equal(x-x, as.array(mat-mat))
expect_equal(as.array(1-mat), as.array(1-mat))
})
test_that("element-wise calculation for matrix", {
x = matrix(1:4, 2, 2)
mat = mx.nd.array(as.array(x), mx.cpu(0))
expect_equal(x, as.array(mat))
expect_equal(x+1, as.array(mat+1))
expect_equal(x-10, as.array(mat-10))
expect_equal(x*20, as.array(mat*20))
expect_equal(x/3, as.array(mat/3), tolerance = 1e-5)
expect_equal(-1-x, as.array(-1-mat))
expect_equal(-5/x, as.array(-5/mat), tolerance = 1e-5)
expect_equal(x+x, as.array(mat+mat))
expect_equal(x/x, as.array(mat/mat))
expect_equal(x*x, as.array(mat*mat))
expect_equal(x-x, as.array(mat-mat))
expect_equal(as.array(1-mat), as.array(1-mat))
})
test_that("ndarray ones, zeros, save and load", {
expect_equal(rep(0, 10), as.array(mx.nd.zeros(10)))
expect_equal(matrix(0, 10, 5), as.array(mx.nd.zeros(c(10, 5))))
expect_equal(rep(1, 10), as.array(mx.nd.ones(10)))
expect_equal(matrix(1, 10, 5), as.array(mx.nd.ones(c(10, 5))))
mat = mx.nd.array(1:20)
mx.nd.save(mat, 'temp.mat')
mat2 = mx.nd.load('temp.mat')
expect_true(is.mx.ndarray(mat2[[1]]))
expect_equal(as.array(mat), as.array(mat2[[1]]))
})
library(testthat)
library(mxnet)
test_check("mxnet")
a <- mx.nd.zeros(c(2, 3)) # create a 2-by-3 matrix on cpu
b <- mx.nd.zeros(c(2, 3), mx.cpu()) # create a 2-by-3 matrix on cpu
c <- mx.nd.zeros(c(2, 3), mx.gpu(0)) # create a 2-by-3 matrix on gpu 0, if you have CUA enabled.
a <- mx.nd.zeros(c(2, 3)) # create a 2-by-3 matrix on cpu
b <- mx.nd.zeros(c(2, 3), mx.cpu()) # create a 2-by-3 matrix on cpu
#c <- mx.nd.zeros(c(2, 3), mx.gpu(0)) # create a 2-by-3 matrix on gpu 0, if you have CUA enabled.
shotSel.dist <<- shot.pt %>%
group_by(ShotDist) %>%
summarise(totalFGA = sum(totalFGA)) %>%
mutate(perc = totalFGA/sum(totalFGA), y.breaks = cumsum(perc) - perc/2) %>%
slice(c(1, 8, 2:7))
shiny::runApp('Dropbox/Data project/NBAstat/app')
View(shotSel.dist)
View(shotSel.def)
shotSel.dist <<- shot.pt %>%
group_by(ShotDist) %>%
summarise(totalFGA = sum(totalFGA)) %>%
mutate(perc = totalFGA/sum(totalFGA), y.breaks = cumsum(perc) - perc/2) %>%
slice(c(1, 8, 2:7))
shiny::runApp('Dropbox/Data project/NBAstat/app')
View(shotSel.dist)
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
rm(list = ls())
shiny::runApp('Dropbox/Data project/NBAstat/app')
shiny::runApp('Dropbox/Data project/NBAstat/app')
library(ggplot2)
library(knitr)
library(dplyr)
data(ToothGrowth)
kable(head(ToothGrowth))
kable(summary(ToothGrowth))
ggplot(ToothGrowth, aes(x = factor(dose), y = len, fill = factor(supp))) +
geom_boxplot() +
ylab('Tooth length') + xlab('Dose in milligrams/day') +
ggtitle('Impact of Dose and Supplement Type') +
theme_bw(base_size = 15) + scale_fill_discrete(name="Supplement type")+
theme(legend.position = c(0.2, 0.8))
anova <- aov(len ~ supp * dose, data=toothGrowth)
summary(anova)
anova <- aov(len ~ supp * dose, data=ToothGrowth)
summary(anova)
TukeyHSD(anova)
TukeyHSD(anova)
anova <- aov(len ~ supp * factor(dose), data=ToothGrowth)
summary(anova)
TukeyHSD(anova)
TukeyHSD(anova)$supp
TukeyHSD(anova)$factor(dose)
TukeyHSD(anova)$supp
TukeyHSD(anova)$'factor(dose)'
TukeyHSD(anova)$supp
TukeyHSD(anova)$'factor(dose)'
?group_bt
?group_by
eq.url <- "http://cmmserv.mrl.illinois.edu/microfabschedule/guest/npMonthView.asp?QM=1&QY=2016&GI=0"
eq <- htmlTreeParse(eq.url, error=function(...){}, useInternalNodes = TRUE)
eq <- xpathSApply(eq,"//select[@name ='GI']/option",xmlValue)
eq.df <- data.frame(GI = seq(0, 38, by = 1), equipment = eq)
head(eq.df)
library(XML)
library(dplyr)
library(ggplot2)
library(knitr)
library(lubridate)
eq.url <- "http://cmmserv.mrl.illinois.edu/microfabschedule/guest/npMonthView.asp?QM=1&QY=2016&GI=0"
eq <- htmlTreeParse(eq.url, error=function(...){}, useInternalNodes = TRUE)
eq <- xpathSApply(eq,"//select[@name ='GI']/option",xmlValue)
eq.df <- data.frame(GI = seq(0, 38, by = 1), equipment = eq)
head(eq.df)
View(eq.df)
library(caret)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(reshape)
#data generation
N <- 200 # number of points per class
D <- 2 # dimensionality
K <- 4 # number of classes
X <- data.frame() # data matrix (each row = single example)
y <- data.frame() # class labels
set.seed(118)
for (j in (1:K)){
r <- seq(0.05,1,length.out = N) # radius
t <- seq((j-1)*4.7,j*4.7, length.out = N) + rnorm(N, sd = 0.3) # theta
Xtemp <- data.frame(x =r*sin(t) , y = r*cos(t)) #np.c_[r*np.sin(t), r*np.cos(t)]
ytemp <- data.frame(matrix(j, N, 1))
X <- rbind(X, Xtemp)
y <- rbind(y, ytemp)
}
data <- data.frame(X, lable = y)
colnames(data) <- c(colnames(X), 'label')
data$label <- factor(data$label)
#creat grid for decision boundary
x_min <- min(X[,1])-0.1
x_max <- max(X[,1])+0.1
y_min <- min(X[,2])-0.1
y_max <- max(X[,2])+0.1
hs <- 0.01
grid <- data.frame(expand.grid(seq(x_min, x_max, by = hs), seq(y_min, y_max, by = hs)))
colnames(grid) <- c('x', 'y')
#train using different models
nnet.fit <- train(label ~ ., data=data, method="nnet", tuneGrid=data.frame(size = 50, decay = 0.01),
trControl=trainControl(method="cv", number=3))
svm.fit.1 <- train(label ~ ., data=data, method="svmLinear",
trControl=trainControl(method="cv", number=3))
svm.fit.2 <- train(label ~ ., data=data, method="svmRadial",
trControl=trainControl(method="cv", number=3))
rf.fit <- train(label ~ ., data=data, method="rf",
trControl=trainControl(method="cv", number=3))
models.name <- c('SVM with linear kernel', 'SVM with RBF kernel', 'Neural network', 'Random forest')
models <- list(svm.fit.1, svm.fit.2, nnet.fit, rf.fit)
#combine predictions on grid
pred <- grid
plot.list <- NULL
for (i in 1:4){
print(i)
Z <- predict(models[[i]], grid)
pred <- cbind(pred, Z)
}
colnames(pred) <- c('x', 'y', models.name)
#convert from wide to long, prepare data for facet in ggplot
pred.melt <- melt(pred, id = c('x', 'y'))
colnames(pred.melt) <- c('x', 'y', 'model', 'label')
#generate decision boundaries
dbPlot <- ggplot(data = pred.melt)+
geom_tile(aes(x = x, y = y,fill = as.character(label)), alpha = 0.3, show.legend = F)+
geom_point(data = data, aes(x=x, y=y, color = as.character(label)), size = 2) +
theme_bw(base_size = 20) + facet_wrap(~model) +
ggtitle('Decision Boundaries')+
theme(axis.ticks=element_blank(), axis.title=element_blank(),
axis.text=element_blank(),legend.position = 'none',
panel.grid.major = element_blank(), panel.grid.minor = element_blank())
#output png
png('dbPlot.png', width=1280, height=800)
dbPlot
dev.off()
#output png
png('dbPlot.png', width=1280, height=800)
dbPlot
dev.off()
setwd("~/Dropbox/Data project/Decision boundary")
#output png
png('dbPlot.png', width=1280, height=800)
dbPlot
dev.off()
png('dbPlot.png', width=1280, height=900)
dbPlot
dev.off()
